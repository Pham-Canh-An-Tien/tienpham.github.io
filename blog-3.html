<!DOCTYPE html>
<html lang="en">

<head>
  <meta charset="utf-8">
  <meta content="width=device-width, initial-scale=1.0" name="viewport">

  <title>Blog</title>
  <meta content="" name="description">
  <meta content="" name="keywords">

  <!-- Favicons -->
  <link href="assets/img/favicon.png" rel="icon">
  <link href="assets/img/apple-touch-icon.png" rel="apple-touch-icon">

  <!-- Vendor CSS Files -->
  <link href="assets/vendor/bootstrap/css/bootstrap.min.css" rel="stylesheet">
  <link href="assets/vendor/bootstrap-icons/bootstrap-icons.css" rel="stylesheet">
  <link href="assets/vendor/glightbox/css/glightbox.min.css" rel="stylesheet">
  <link href="assets/vendor/swiper/swiper-bundle.min.css" rel="stylesheet">

  <!-- Template Main CSS File -->
  <link href="assets/css/style.css" rel="stylesheet">

  <!-- =======================================================
  * Template Name: DevFolio - v4.8.1
  * Template URL: https://bootstrapmade.com/devfolio-bootstrap-portfolio-html-template/
  * Author: BootstrapMade.com
  * License: https://bootstrapmade.com/license/
  ======================================================== -->
</head>

<body>

  <!-- ======= Header ======= -->
  <header id="header" class="fixed-top">
    <div class="container d-flex align-items-center justify-content-between">

      <h1 class="logo"><a href="index.html">Tien Pham</a></h1>
      <!-- Uncomment below if you prefer to use an image logo -->
      <!-- <a href="index.html" class="logo"><img src="assets/img/logo.png" alt="" class="img-fluid"></a>-->

      <nav id="navbar" class="navbar">
        <ul>
          <li><a class="nav-link scrollto active" href="#hero">Home</a></li>
          <li><a class="nav-link scrollto" href="#about">About</a></li>
          <li><a class="nav-link scrollto" href="#services">Education</a></li>
          <li><a class="nav-link scrollto " href="#work">Project</a></li>
          <li><a class="nav-link scrollto " href="#blog">Publications</a></li>
          <li><a class="nav-link scrollto" href="#contact">Contact</a></li>
        </ul>
        <i class="bi bi-list mobile-nav-toggle"></i>
      </nav><!-- .navbar -->

    </div>
  </header><!-- End Header -->

  <div class="hero hero-single route bg-image" style="background-image: url(assets/img/background.jpg)">
    <div class="overlay-mf"></div>
    <div class="hero-content display-table">
      <div class="table-cell">
        <div class="container">
          <h2 class="hero-title mb-4">Text Classification using Naive Bayes Classifier</h2>
        </div>
      </div>
    </div>
  </div>

  <main id="main">

    <!-- ======= Blog Single Section ======= -->
    <section class="blog-wrapper sect-pt4" id="blog">
      <div class="container">
        <div class="row">
          <div class="col-md-12">
            <div class="post-box">
              <div class="post-thumb">
                <!-- <img src="assets/img/kaggle1/titanic.jpg" class="img-fluid" alt=""> -->
              </div>
              <div class="post-meta">
                <h1 class="article-title">Text Classification using Naive Bayes Classifier
                </h1>
                <ul>
                  <li>
                    <span class="bi bi-person"></span>
                    <a href="/index.html">Tien Pham</a>
                  </li>
                  <li>
                    <span class="bi bi-tag"></span>
                    <a href="https://www.kaggle.com/code/tienapham/draft-10-05/data?scriptVersionId=107442102">Text Classification</a>
                  </li>
                  <li>
                    <span class="bi bi-tag"></span>
                    <a href="https://www.kaggle.com/code/tienapham/draft-10-05/data?scriptVersionId=107442102">Naive Bayes Classifier</a>
                  </li>
                </ul>
              </div>
              <div class="article-content">
                <h1 class="article-title">Introduction</h1>
                <p>
                  Naive Bayes Theorem is one of the most famous theorem governing for modern statistical problem and hence, it also contributes to 
                  the development of Machine Learning. In this tutorial, we will introduce the Bayes theorem and how to apply it for classification problems.
                  For those who are not familiar with the platform we use in this tutorial including Google Colab and Jupiter Notebook, please refer to my 
                  previous <a href="blog-2.html"><b>Blog</b></a>.
                </p>
                <h1 class="article-title">Bayes Theorem</h1>
                <p>
                 <a href="https://en.wikipedia.org/wiki/Bayes%27_theorem"><b>Bayes Theorem</b></a> descibe the probability of an event based on prior knowledge,
                 which is different with frequencist who don't consider the prior knowledge. The governance equation of Bayes Theorem is conditional probability:
                 <img src="assets/img/kaggle3/1.jpg" width = "1000">
                </p>
                <p>
                  As a classification perspective, if we can calculate the posibility of the sample into each class, we would classify that sample to the highest posibility class
                 Therefore, Bayes Classifier is known as probabilistic classifier. However, Bayes Theorem is established under the assumption that all the attributes are
                 <b> conditionally independence</b>. That is why the word <b>Naive</b> come to.
                 <img src="assets/img/kaggle3/2.png" width = "1000">
                 <h6>https://jagan-singhh.medium.com/naive-bayes-classifier-99e3e618f8db</h6>
                </p>
                <p>
                  Naive Bayes Classifier is famous for Natural Language Processing, especially in Text Classification where the feature matrices are very sparse which is 
                  problematic in other Machine Learning algorithm such as Support Vector Machine (SVM), Random Forest, and so on. In order to demonstrate how Naive Bayes Classifier works
                  in detail, let go though the step-by-step example.
                </p>
                <h1 class="article-title">Dataset</h1>
                At first, we use <a href="https://www.kaggle.com/datasets/gaveshjain/ford-sentence-classifiaction-dataset?resource=downloa">Ford Sentence Classification</a> where
                the sentences are clasified into 6 classes.
                <img src="assets/img/kaggle3/3.jpg" width = "1000">
                <p>
                  However, the dataset only has train set and test set, and the test set does not has class attributes in the table, we have to infer from the sentence_id.
                  <img src="assets/img/kaggle3/4.jpg" width = "1000">
                </p>
                <p>
                  We also need to tokenize the classes into number so that it is easier for later computation process. We will introduce new colum called "class" in the data
                  <img src="assets/img/kaggle3/5.jpg" width = "1000">
                  <img src="assets/img/kaggle3/6.jpg" width = "1000">
                </p>
                
                <p>
                  After merging the data, we now spit the data into train set, dev set and test set. The common practice is 6:2:2 where 60% of data is used for training,
                  20% is used for developing the model and the remaining 20% is for testing.
                  <img src="assets/img/kaggle3/7.jpg" width = "1000">
                </p>
                <h1 class="article-title">Build the model</h1>
                <p>
                  In order to build the Naive Bayes Classifier, we have to define the vocabulary of text which we determine as attributes for calculation. The vocabulary should
                  have some information relating to the classes that we want to predict. However, there are several words in our dataset that is hard to analyze word by word.
                  However, there are some word that is meaningless in the English context such as "the", "a","of", "they", etc. That is to say, not all of the words
                  has the meaning to classification problem. Those meaningless words are called <b>Stop Words</b> during the feature extraction process where we split 
                  the text into multiple words and consider their existances as features for classification. That is to say, those words exist with the high requency in the text
                  but has no or little information, we call this <a href="https://en.wikipedia.org/wiki/Tf%E2%80%93idf"><b>Inverse Document Frequency (IDF)</b></a>.The English 
                  stop words are defined by many libraries. However, we should define our stop words so that we can add or remove the words that are meaningfull or meaningless to the problems.
                </p>
                <img src="assets/img/kaggle3/8.jpg" width = "1000">
                <p>
                  After this, we start processing the training set to build the vocabulary list. The first thing is to split the sentence into words with the predefined delimiter.
                  In this exmple, I use " ", "," and "." as the delimiter.
                </p>
                <img src="assets/img/kaggle3/9.jpg" width = "1000">
                <p>
                  Let see how many vocabulary is extracted and their counts
                </p>
                <img src="assets/img/kaggle3/10.jpg" width = "1000">
                <p>
                  There are more than 30000 vocabs with there fequency varies. That means for some word such as "Strong", there are very common in the data while the word "Workplace" <isindex>
                    very little (only 5 out of 60000 samples in training set). For better computational performance, we will remove some rare words that is exists less than 5 times
                    in the trainig data.
                </p>
                <img src="assets/img/kaggle3/11.jpg" width = "1000">
                <p>
                  We actually reduce the vocabulary size 5 times, which significantly reduce the computational cost. For easier for later process, we will tokenize the position of the words,
                  in the features set.
                </p>
                <img src="assets/img/kaggle3/12.jpg" width = "1000">
                <p>
                  Now we start moving to calculating the probability of each word given the classes. First of all, we will accumulate the existance of the word in each class. We also count
                  how many sample for each class in training set which are use to calculate the probability of each class. Note that a lot of values are 0 which is problematics in NBC
                  where the probability would be always 0 beause of the lack of existance in training set.
                </p>
                <img src="assets/img/kaggle3/13.jpg" width = "1000">
                <p>
                  Then we can calculate the probability of each word given the class they are in. This should be equal to the number of words in that class divided by the number
                  of sample of this class. And the probability of each class equal to it existance divided by total samples
                </p>
                <img src="assets/img/kaggle3/14.jpg" width = "1000">
                <p>
                  Now we can extract the developement dataset as the same way as process the training set
                </p>
                <img src="assets/img/kaggle3/15.jpg" width = "1000">
                <p>
                  In order to predict the class, we need to calculate all the probability of each class given the sample
                </p>
                <img src="assets/img/kaggle3/16.jpg" width = "1000">
                <p>
                  Then we will go through the dev dataset and get the performance
                </p>
                
                <img src="assets/img/kaggle3/17.jpg" width = "1000">
                <p>
                  The performance is really bad (~26%), but let see how we can improve it using Laplace Smoothing. With the Laplace Smoothing, we will
                  add one more samples for each word in each class, which will give some small possibility for the word in class event the training set does not
                  have it. 
                </p>
                <img src="assets/img/kaggle3/19.jpg" width = "1000">
                <img src="assets/img/kaggle3/18.jpg" width = "1000">
                <p>
                  With the Laplace Smoothing, let's see how the model predict in our dev set.
                </p>
                <img src="assets/img/kaggle3/20.jpg" width = "1000">
                <p>
                  With the Laplace Smoothing, the performance is slightly increased to ~27%. Let see how it perform with test set.
                </p>
                <img src="assets/img/kaggle3/21.jpg" width = "1000">
                <p>
                  Although the performance is really low, but it is consistent from devset and testset and still better than random guess (~17%). We still have many rooms
                  to improve the model in the Challenges and Solution Section
                </p>
                  <h1 class="article-title">Conclusion</h1>
                  <p>
                    This blog shows you how to build a basic Naive Bayes Classifier for text classification in Google Colab. The code for this tutorial can be found <a href="https://github.com/Pham-Canh-An-Tien/data-mining1/blob/master/naive_bayes_text_clasifier.ipynb"><b>here</b></a>.
                    The dataset is not well processed and we mostly learn how to process the data, especially in text classifier. Although the performance is really low, I think it's mostly
                    because the vocabulary is not good enough and the dataset is very difficult (the word in each sentence is too little that give us too little information)
                  </p>
                  <h1 class="article-title">Challenges and Solution</h1>
                  <p>
                    I have been facing many challenges in this homework. Some of them have been solved but I could not make it for all of problems. The challenges and proposed
                    solution are below:
                    <li>
                      The test dataset is <b>not labelled</b>. However, I create a script to label the test data base on the sentence_id. It tooks a lot of time to 
                    recognise such that problem.
                    </li>
                    <li>
                      The test dataset is <b>not labelled</b>. However, I create a script to label the test data base on the sentence_id. It tooks a lot of time to 
                    recognise such that problem.
                    </li>
                    <li>
                      The dataset is <b>not balanced</b>. We can either split with the same ratio for training data or train the model with weighted traning. I could not find times
                      to make it.
                    </li>
                    <li>
                      Some of sentences are too short (only 3 words) which make it difficult to classifiy it (don't have enough information).
                    </li>
                    <li>
                      The vocabulary based on the training set with no meaning consideration is not an ideal case. Furthermore, the word that are not related to
                      those classes are not well removed. We can investigate more on the stop words to remove as much non-related words as possible. It could help us
                      to improve the perfomance in term of accuracy and computational cost.
                    </li>
                  </p>
                  <h1 class="article-title">Contribution</h1>
                  <p>
                    The Notebook have been developed by Tien Pham with no reference from any code (which is very time consuming to understand the theory, build the code
                    , debug and try with different approach). My contributions are listed below:
                    <li>
                      I tried to understand the flow of Naive Bayes Classifier and design the data structure to build the model</b>.
                    </li>
                    <li>
                      I do design the model from scratch with the experiments on how smoothing affect on the performance of the model.
                    </li>
                    <li>
                      I wrote the script to process the data including labelling the test dataset, tokenize the classes.
                    </li>
                    <li>
                      I wrote all the functions to process the sentences, build the vocabulary list, remove the rare words, etc..
                    </li>
                    <li>
                      A full tutorial for beginners for text classification using Naive Bayes Classifier with explaination.
                    </li>
                  </p>
                  <h1 class="article-title">References</h1>
                  <p>
                    [1].Rish, Irina. "An empirical study of the naive Bayes classifier." IJCAI 2001 workshop on empirical methods in artificial intelligence. Vol. 3. No. 22. 2001..
                  </p>
                  <p>
                    [2]. Xu, Shuo, Yan Li, and Zheng Wang. "Bayesian multinomial Naïve Bayes classifier to text classification." Advanced multimedia and ubiquitous engineering. Springer, Singapore, 2017. 347-352..</a>
                  </p>
                  <p>
                    [3]. Xu, Shuo. "Bayesian Naïve Bayes classifiers to text classification." Journal of Information Science 44.1 (2018): 48-59.</a>
                  </p>
                  <p>
                    [4]. Zhang, Wei, and Feng Gao. "An improvement to naive bayes for text classification." Procedia Engineering 15 (2011): 2160-2164.</a>
                  </p>
              </div>
            </div>
          
        
        </div>
      </div>
    </section><!-- End Blog Single Section -->

  </main><!-- End #main -->

  <!-- ======= Footer ======= -->
  <footer>
    <div class="container">
      <div class="row">
        <div class="col-sm-12">
          <div class="copyright-box">
            <p class="copyright">&copy; Copyright <strong>Tien Pham</strong>. All Rights Reserved</p>
            <div class="credits">
              <!--
              All the links in the footer should remain intact.
              You can delete the links only if you purchased the pro version.
              Licensing information: https://bootstrapmade.com/license/
              Purchase the pro version with working PHP/AJAX contact form: https://bootstrapmade.com/buy/?theme=DevFolio
            -->
              <!-- Designed by <a href="https://bootstrapmade.com/">BootstrapMade</a> -->
            </div>
          </div>
        </div>
      </div>
    </div>
  </footer><!-- End  Footer -->

  <div id="preloader"></div>
  <a href="#" class="back-to-top d-flex align-items-center justify-content-center"><i class="bi bi-arrow-up-short"></i></a>

  <!-- Vendor JS Files -->
  <script src="assets/vendor/purecounter/purecounter_vanilla.js"></script>
  <script src="assets/vendor/bootstrap/js/bootstrap.bundle.min.js"></script>
  <script src="assets/vendor/glightbox/js/glightbox.min.js"></script>
  <script src="assets/vendor/swiper/swiper-bundle.min.js"></script>
  <script src="assets/vendor/typed.js/typed.min.js"></script>
  <script src="assets/vendor/php-email-form/validate.js"></script>

  <!-- Template Main JS File -->
  <script src="assets/js/main.js"></script>

</body>

</html>